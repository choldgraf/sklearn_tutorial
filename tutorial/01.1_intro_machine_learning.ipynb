{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<small><i>This notebook was put together by [Jake Vanderplas](http://www.vanderplas.com). Source and license info is on [GitHub](https://github.com/jakevdp/sklearn_tutorial/).</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Scikit-Learn: Machine Learning with Python\n",
    "\n",
    "This session will cover the basics of Scikit-Learn, a popular package containing a collection of tools for machine learning written in Python. See more at http://scikit-learn.org."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "**Main Goal:** To introduce the central concepts of machine learning, and how they can be applied in Python using the Scikit-learn Package.\n",
    "\n",
    "- Definition of machine learning\n",
    "- Data representation in scikit-learn\n",
    "- Introduction to the Scikit-learn API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages we'll use\n",
    "## Scikit-Learn (sklearn)\n",
    "[Scikit-Learn](http://github.com/scikit-learn/scikit-learn) is a Python package designed to give access to **well-known** machine learning algorithms within Python code, through a **clean, well-thought-out API**. It has been built by hundreds of contributors from around the world, and is used across industry and academia.\n",
    "\n",
    "Scikit-Learn is built upon Python's [NumPy (Numerical Python)](http://numpy.org) and [SciPy (Scientific Python)](http://scipy.org) libraries, which enable efficient in-core numerical and scientific computation within Python. As such, scikit-learn is not specifically designed for extremely large datasets, though there is [some work](https://github.com/ogrisel/parallel_ml_tutorial) in this area.\n",
    "\n",
    "For this short introduction, I'm going to stick to questions of in-core processing of small to medium datasets with Scikit-learn.\n",
    "\n",
    "\n",
    "## Seaborn / Matplotlib\n",
    "These are both packages for data visualization in python. [Matplotlib](http://matplotlib.org/) provides the base plotting package that many visualizations build on top of, and [Seaborn](https://web.stanford.edu/~mwaskom/software/seaborn/) is a handy way to make your plots look much cleaner, relatively quickly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# set seaborn plot defaults.\n",
    "# This can be safely commented out\n",
    "import seaborn; seaborn.set()\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Machine Learning?\n",
    "\n",
    "In this section we will begin to explore the basic principles of machine learning.\n",
    "Machine Learning is about building programs with **tunable parameters** (typically an\n",
    "array of floating point values) that are adjusted automatically so as to improve\n",
    "their behavior by **adapting to previously seen data.**\n",
    "\n",
    "Machine Learning can be considered a subfield of **Artificial Intelligence** since those\n",
    "algorithms can be seen as building blocks to make computers learn to behave more\n",
    "intelligently by somehow **generalizing** rather that just storing and retrieving data items\n",
    "like a database system would do.\n",
    "\n",
    "Machine learning is all about finding **patterns** in data, and using those patterns to do useful things. Sometimes we want to investigate what kinds of patterns were found, other times we just want to use those patterns to make predictions about new data.\n",
    "\n",
    "## Types of machine learning problems\n",
    "There are two primary types of problems in machine learning. These are:\n",
    "* **Classification** - attempting to predict a *categorical* variable from one or many inputs.\n",
    "* **Regression** - attempting to predict a *continuous* variable from one or many inputs.\n",
    "\n",
    "### Classification\n",
    "We'll first address **classification**. Say that we've collected two variables of interest from a population, and we know that there are two types of people in this population. As such, we have 3 types of information per person:\n",
    "\n",
    "1. Their value for feature 1\n",
    "2. Their value for feature 2\n",
    "3. The group they belong to (their \"class\" in ML terms).\n",
    "\n",
    "We can plot these values below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll suppress warnings so it doesn't clutter the output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the example plot from the figures directory\n",
    "from fig_code import plot_sgd_separator\n",
    "plot_sgd_separator()\n",
    "ax = plt.gca()\n",
    "_ = plt.setp(ax.collections[:-1], visible=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **classification** algorithm asks \"given this data, can I draw a *line* through this plot that will reliably divide one group from another?\" Do you think this is possible?\n",
    "\n",
    "Here's an example of one such line. We'll show the dividing line it self, along with a \"padding\" around it that tries to keep an equal distance between the line and the center of each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = plt.setp(ax.collections[:-1], visible=True)\n",
    "ax.figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most classification algorithms are variations on how to find the \"right\" line to separate groups, given the data**.\n",
    "\n",
    "Importantly, the line shown above represents a more abstract assumption about the structure that our datapoints follow. We are formally modeling the difference between these two classes. This lets us **generalize** to new data: if you were to drop another point onto the plane which is unlabeled, this algorithm could now **predict** whether it's a blue or a red point.\n",
    "\n",
    "While this example is simple, imagine if we had a third dimension. Or a fourth, fifth, sixth, or six hundredth dimension. Imagine if the groups were overlapping in some places. What would be the best way to draw a line between the groups now? Much research in machine learning aims to answer this question.\n",
    "\n",
    "*Note: If you'd like to see the source code used to generate this, you can either open the code in the `figures` directory, or you can load the code using the `%load` magic command.*\n",
    "\n",
    "### Regression\n",
    "The next simple task we'll look at is a **regression** task. In this case, we wish to predict a *continuous* output, given some inputs. For example, let's say we collect two variables, and we want to know the *relationship* between them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fig_code import plot_linear_regression\n",
    "plot_linear_regression()\n",
    "ax = plt.gca()\n",
    "_ = plt.setp(ax.lines, visible=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the relationship between `x` and `y`? In this case, it looks like one could be **modeled** as a linear function of the other. This just means that we can draw a line through this cloud of points, and it would do a good job of explaining the relationship between them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = plt.setp(ax.lines, visible=True)\n",
    "ax.figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to classification, by fitting a line to the data we are modeling the underlying relationship that exists between `x` and `y`. However, in the case of regression, we are choosing a line that maximizes the chance to correctly predict `y`, given `x`.\n",
    "\n",
    "The model has **learned** from the training data, and can be used to **generalize** to new data by predicting new values for `y` given a value of `x` that it hasn't seen before.\n",
    "\n",
    "> **What is the main difference between classification and regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizing Machine Learning in Python\n",
    "As we've discussed above, the goal of machine learning is to **learn** the underlying structure in the data to accomplish a given task (either *classification* or *regression*). There are hundreds of algorithms to do this, and just as many packages that implement these algorithms.\n",
    "\n",
    "This tutorial focuses on the most flexible and user-friendly package for doing machine learning in Python: **scikit-learn**. \n",
    "## Representation of Data in Scikit-learn\n",
    "\n",
    "Machine learning is about creating models from data: for that reason, we'll start by discussing how data can be represented in order to be understood by the computer.  Along with this, we'll build on our matplotlib examples from the previous section and show some examples of how to visualize data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most machine learning algorithms implemented in scikit-learn expect data to be stored in a **two-dimensional array or matrix**.  The arrays can be\n",
    "either ``numpy`` arrays, or in some cases ``scipy.sparse`` matrices.\n",
    "\n",
    "The size of the array is expected to be `[n_samples, n_features]`\n",
    "\n",
    "- **n_samples:**   The number of samples: each sample is an item to process (e.g. classify).\n",
    "  A sample can be a document, a picture, a sound, a video, an astronomical object,\n",
    "  a row in database or CSV file,\n",
    "  or whatever you can describe with a fixed set of quantitative traits.\n",
    "- **n_features:**  The number of features or distinct traits that can be used to describe each\n",
    "  item in a quantitative manner.  Features are generally real-valued, but may be boolean or\n",
    "  discrete-valued in some cases.\n",
    "\n",
    "The number of features must be fixed in advance. However it can be very high dimensional (e.g. millions of features) with most of them being zeros for a given sample. This is a case where `scipy.sparse` matrices can be useful, in that they are much more memory-efficient than numpy arrays.\n",
    "\n",
    "> **What kind of data would we expect as an output in **classification** algorithms?**\n",
    "\n",
    "> **What about **regression** algorithms?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Example: the Iris Dataset\n",
    "\n",
    "There are a number of \"toy\" problems in machine learning that we can use to illustrate its principles. One of the most famous is the \"Iris\" dataset\n",
    "As an example of a simple dataset, we're going to take a look at the\n",
    "iris data stored by scikit-learn.\n",
    "The data consists of measurements of three different species of irises.\n",
    "There are three species of iris in the dataset, which we can picture here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import Image, display\n",
    "display(Image(filename='images/iris_setosa.jpg'))\n",
    "print(\"Iris Setosa\\n\")\n",
    "\n",
    "display(Image(filename='images/iris_versicolor.jpg'))\n",
    "print(\"Iris Versicolor\\n\")\n",
    "\n",
    "display(Image(filename='images/iris_virginica.jpg'))\n",
    "print(\"Iris Virginica\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **If we want to design an algorithm to recognize iris species, what might the data be?**\n",
    "\n",
    "Remember: we need a 2D array of size `[n_samples x n_features]`.\n",
    "\n",
    "> **What would the `n_samples` refer to?**\n",
    "\n",
    "> **What might the `n_features` refer to?**\n",
    "\n",
    "Remember that there must be a **fixed** number of features for each sample, and feature number ``i`` must be a similar kind of quantity for each sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Iris Data with Scikit-Learn\n",
    "\n",
    "Scikit-learn has a very straightforward set of data on these iris species.  The data consist of\n",
    "the following:\n",
    "\n",
    "- Features in the Iris dataset:\n",
    "\n",
    "  1. sepal length in cm\n",
    "  2. sepal width in cm\n",
    "  3. petal length in cm\n",
    "  4. petal width in cm\n",
    "\n",
    "- Target classes to predict:\n",
    "\n",
    "  1. Iris Setosa\n",
    "  2. Iris Versicolour\n",
    "  3. Iris Virginica\n",
    "  \n",
    "``scikit-learn`` embeds a copy of the iris CSV file along with a helper function to load it into numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples, n_features = iris.data.shape\n",
    "print((n_samples, n_features))\n",
    "print(iris.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(iris.data.shape)\n",
    "print(iris.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These are the target \"classes\", aka the species of each flower.\n",
    "print(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is four dimensional, but we can visualize two of the dimensions\n",
    "at a time using a simple scatter-plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, we'll show the raw data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_index = 1\n",
    "y_index = 2\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax.scatter(iris.data[:, x_index], iris.data[:, y_index])\n",
    "ax.set_xlabel(iris.feature_names[x_index])\n",
    "ax.set_ylabel(iris.feature_names[y_index]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this formatter will label the colorbar with the correct target names\n",
    "formatter = plt.FuncFormatter(lambda i, *args: iris.target_names[int(i)])\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "sct = ax.scatter(iris.data[:, x_index], iris.data[:, y_index],\n",
    "                 c=iris.target, cmap=plt.cm.get_cmap('RdYlBu', 3),\n",
    "                 vmin=-.5, vmax=2.5)\n",
    "f.colorbar(sct, ticks=[0, 1, 2], format=formatter)\n",
    "ax.set_xlabel(iris.feature_names[x_index])\n",
    "ax.set_ylabel(iris.feature_names[y_index]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Do you think it's possible to draw a line separating each class from the other two classes?**\n",
    "\n",
    "> **Change** `x_index` **and** `y_index` **in the above script\n",
    "and find a combination of two parameters\n",
    "which maximally separate the three classes.**\n",
    "\n",
    "This exercise is a preview of **dimensionality reduction**, which we'll see later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Available Data\n",
    "A big part of machine learning is finding the right dataset for your question. Fortunately, scikit-learn has several datasets to make it easier to learn these methods. They come in three flavors:\n",
    "\n",
    "- **Packaged Data:** these small datasets are packaged with the scikit-learn installation,\n",
    "  and can be downloaded using the tools in ``sklearn.datasets.load_*``\n",
    "- **Downloadable Data:** these larger datasets are available for download, and scikit-learn\n",
    "  includes tools which streamline this process.  These tools can be found in\n",
    "  ``sklearn.datasets.fetch_*``\n",
    "- **Generated Data:** there are several datasets which are generated from models based on a\n",
    "  random seed.  These are available in the ``sklearn.datasets.make_*``\n",
    "\n",
    "You can explore the available dataset loaders, fetchers, and generators using IPython's\n",
    "tab-completion functionality.  After importing the ``datasets`` submodule from ``sklearn``,\n",
    "type\n",
    "\n",
    "    datasets.load_ + TAB\n",
    "\n",
    "or\n",
    "\n",
    "    datasets.fetch_ + TAB\n",
    "\n",
    "or\n",
    "\n",
    "    datasets.make_ + TAB\n",
    "\n",
    "to see a list of available functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Type datasets.fetch_<TAB> or datasets.load_<TAB> in IPython to see all possibilities\n",
    "\n",
    "# datasets.fetch_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# datasets.load_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we'll use some of these datasets and take a look at the basic principles of machine learning."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
